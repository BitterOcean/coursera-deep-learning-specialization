# Sequence Models

**by** <a href="https://DeepLearning.AI/">DeepLearning.AI</a>

**Taught by**: <a href="https://www.coursera.org/instructor/andrewng">Andrew Ng</a>

**Student**: <a href="https://maryamsaeedmehr.github.io/">Maryam Saeidmehr</a>

## About this Course

In the fifth course of the Deep Learning Specialization, you will become familiar with sequence models and their exciting applications such as speech recognition, music synthesis, chatbots, machine translation, natural language processing (NLP), and more. 

By the end, you will be able to build and train Recurrent Neural Networks (RNNs) and commonly-used variants such as GRUs and LSTMs; apply RNNs to Character-level Language Modeling; gain experience with natural language processing and Word Embeddings; and use HuggingFace tokenizers and transformer models to solve different NLP tasks such as NER and Question Answering.

The Deep Learning Specialization is a foundational program that will help you understand the capabilities, challenges, and consequences of deep learning and prepare you to participate in the development of leading-edge AI technology. It provides a pathway for you to take the definitive step in the world of AI by helping you gain the knowledge and skills to level up your career.

| Item | Description |
|---|---|
| üìì Basic Info  |  Course 5 of 5 in the <a href="https://www.coursera.org/specializations/deep-learning">Deep Learning Specialization</a>  |
| ‚öñÔ∏è Level  | Intermediate  |
| ‚è∞ Commitment  | At the rate of 5 hours a week, it takes roughly 5 weeks to finish each course in the Specialization.  |
| üî§ Language  | English, **Subtitles**: Chinese (Traditional), Arabic, French, Ukrainian, Portuguese (European), Chinese (Simplified), Italian, Portuguese (Brazilian), Vietnamese, Korean, German, Russian, Turkish, Spanish, Japanese</br> <a href="https://www.coursera.org/learn/neural-networks-deep-learning/home/info#">Volunteer to translate subtitles for this course</a>  |
| üßë‚Äçüéì How To Pass	Pass  |  all graded assignments to complete the course. |
| ‚≠ê User Ratings  | ![Rating](https://img.shields.io/badge/rating-4.9-brightgreen) |

## Syllabus

- <details open><summary><h2>Week 1</h2></summary>

  ### Recurrent Neural Networks
    Discover recurrent neural networks, a type of model that performs extremely well on temporal data, and several of its variants, including LSTMs, GRUs and Bidirectional RNNs,

    <details>
      <summary>üìÇ 12 videos, 4 reading</summary>

  - Video: Why Sequence Models?
  - Video: Notation
  - Video: Recurrent Neural Network Model
  - Video: Backpropagation Through Time
  - App Item: [IMPORTANT] Have questions, issues or ideas? Join our Community!
  - Video: Different Types of RNNs
  - Video: Language Model and Sequence Generation
  - Video: Sampling Novel Sequences
  - Video: Vanishing Gradients with RNNs
  - Reading: Clarifications about Upcoming Gated Recurrent Unit (GRU) Video
  - Video: Gated Recurrent Unit (GRU)
  - Reading: Clarifications about Upcoming Long Short Term Memory (LSTM) Video
  - Video: Long Short Term Memory (LSTM)
  - Video: Bidirectional RNN
  - Video: Deep RNNs
  - Reading: Lecture Notes W1
  - Reading: (Optional) Downloading your Notebook, Downloading your Workspace and Refreshing your Workspace

    </details>

    üî¨**Graded**: Recurrent Neural Networks

    üî¨**Graded**: Building your Recurrent Neural Network - Step by Step

    üî¨**Graded**: Dinosaur Island-Character-Level Language Modeling

    üî¨**Graded**: Jazz Improvisation with LSTM

  </details>
---  
  - <details open><summary><h2>Week 2</h2></summary>

    ### Natural Language Processing & Word Embeddings
      Natural language processing with deep learning is a powerful combination. Using word vector representations and embedding layers, train recurrent neural networks with outstanding performance across a wide variety of applications, including sentiment analysis, named entity recognition and neural machine translation.

      <details>
        <summary>üìÇ 10 videos, 2 readings</summary>

    - Video: Word Representation
    - Video: Using Word Embeddings
    - Video: Properties of Word Embeddings
    - Video: Embedding Matrix
    - Video: Learning Word Embeddings
    - Video: Word2Vec
    - Video: Negative Sampling
    - Reading: Clarifications about Upcoming GloVe Word Vectors Video
    - Video: GloVe Word Vectors
    - Video: Sentiment Classification
    - Video: Debiasing Word Embeddings
    - Reading: Lecture Notes W2

      </details>

      üî¨**Graded**: Natural Language Processing & Word Embeddings
      
      üî¨**Graded**: Operations on Word Vectors - Debiasing
      
      üî¨**Graded**: Emojify

    </details>
---
  - <details open><summary><h2>Week 3</h2></summary>

    ### Sequence Models & Attention Mechanism
      Augment your sequence models using an attention mechanism, an algorithm that helps your model decide where to focus its attention given a sequence of inputs. Then, explore speech recognition and how to deal with audio data.

      <details>
        <summary>üìÇ 10 videos, 3 readings</summary>

    - Video: Basic Models
    - Video: Picking the Most Likely Sentence
    - Video: Beam Search
    - Video: Refinements to Beam Search
    - Video: Error Analysis in Beam Search
    - Video: Bleu Score (Optional)
    - Video: Attention Model Intuition
    - Reading: Clarifications about Upcoming Attention Model Video
    - Video: Attention Model
    - Video: Speech Recognition
    - Video: Trigger Word Detection
    - Reading: Lecture Notes W3
    - Reading: Instructions If You Are Unable to Open Your Notebook

      </details>

      üî¨**Graded**: Sequence Models & Attention Mechanism
      
      üî¨**Graded**: Neural Machine Translation
      
      üî¨**Graded**: Trigger Word Detection

    </details>
---
  - <details open><summary><h2>Week 4</h2></summary>

    ### Transformer Network

      <details>
        <summary>üìÇ 5 videos, 5 readings</summary>

    - Video: Transformer Network Intuition
    - Video: Self-Attention
    - Video: Multi-Head Attention
    - Video: Transformer Network
    - Reading: Lecture Notes W4
    - Ungraded Lab: Transformer Pre-processing
    - Ungraded Lab: Transformer Network Application: Named-Entity Recognition
    - Ungraded Lab: Transformer Network Application: Question Answering
    - Reading: Transformers using Trax Library
    - Video: Conclusion and Thank You!
    - Reading: References
    - Reading: Acknowledgments
    - Reading: (Optional) Opportunity to Mentor Other Learners

      </details>

      üî¨**Graded**: Transformers
      
      üî¨**Graded**: Transformers Architecture with TensorFlow

    </details>

## Certificate

  ![Screenshot from 2023-04-24 18-12-23](https://user-images.githubusercontent.com/60509979/234031148-d8699338-b369-4c24-83a8-45df6f157b27.png)
  
  - ![IssuingOrganization](https://img.shields.io/badge/Issuing%20Organization-Coursera-informational)
  - ![IssueDate](https://img.shields.io/badge/Issue%20Date-Sept%202023-informational)
  - ![CredentialID](https://img.shields.io/badge/Credential%20ID-Q7HW9L4BYSHE-informational)
  - <a href="https://coursera.org/share/6d2856b38ad0187c4d20fe08001e16f4">![CredentialLink](https://img.shields.io/badge/Credential%20Link-https://coursera.org/share/6d2856b38ad0187c4d20fe08001e16f4)</a>
  - <a href="https://github.com/BitterOcean/coursera-deep-learning-specialization/blob/main/Neural-Networks-and-Deep-Learning/Certificate.pdf">![CertificateDownload](https://img.shields.io/badge/Certificate-Download%20PDF-informational)</a>
